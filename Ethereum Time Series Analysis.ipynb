{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import ruptures as rpt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.optimizers import Adam, Adamax, RMSprop\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "colors = sns.color_palette('deep', 8)\n",
    "sns.set_palette(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethereum Time Series Analysis\n",
    "1. Introduction\n",
    "    - Imports:\n",
    "        - Python: datetime, time, warnings\n",
    "        - Preprocessing/Viz: numpy, pandas, matplotlib, seaborn, sklearn.preprocessing, ruptures\n",
    "        - Modeling: statsmodels.tsa.arima_model, tensorflow, keras.models, .layers, .backend\n",
    "        - Evaluation: statsmodels.tsa.stattools, scipy.stats, sklearn.metrics\n",
    "    - Description of Research Question/Task\n",
    "2. Data: TS Only\n",
    "    - Scrape/APIs/Clean\n",
    "    - Load/EDA\n",
    "        - Visualize TS: ADF (Augmented Dickey-Fuller) Unit Root Test\n",
    "        - Distribution: D'Agostino/Pearson Normality Test\n",
    "    - Changepoint Analysis\n",
    "        - PELT (Pruned Exact Linear Time)\n",
    "    - Feature Engineering: changepoint dummies\n",
    "3. Modeling/Forecasting\n",
    "    - ARIMA\n",
    "        - Stationarity: ADF\n",
    "        - AR/MA (Auto Regressive/Moving Average): ACF/PACF (Auto/Partial Auto Correlation Functions)\n",
    "        - Modeling: AIC/BIC (Akaike/Bayesian Information Critera), RMSE (Root Mean Squared Error)\n",
    "        - Forecasting: Moving window, RMSE\n",
    "    - Neural Networks\n",
    "        - MLP ANN (Multi Layer Perceptron Artificial Neural Network): simple, deep, RMSE\n",
    "        - LSTM RNN (Long Short-Term Memory Recurrent Neural Network): RMSE\n",
    "    - Visualize/Summarize Results\n",
    "5. Data: Exogenous Variables\n",
    "    - Scrape/APIs/Clean\n",
    "    - Granger Causality\n",
    "    - Feature Engineering: Collinearity, PCA (Principle Component Analysis), UMAP(?) (Uniform Manifold Approximation and Projection)\n",
    "6. Modeling/Forecasting\n",
    "    - ARIMA(X)\n",
    "    - Neural Networks\n",
    "        - MLP ANN\n",
    "        - LSTM RNN\n",
    "    - Visualize/Summarize Results\n",
    "7. Overall Summary & Conclusions\n",
    "8. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cryptocompare_data(symbol,\n",
    "                       comparison_symbol='USD',\n",
    "                       all_data=True,\n",
    "                       rows=1,\n",
    "                       aggregate=1,\n",
    "                       write_to=False):\n",
    "    \n",
    "    # symbol = symbol of desired coin\n",
    "    # comparison symbol = symbol for currency to price with\n",
    "    # to subset data, set all_data to false, then specificy with rows parameter\n",
    "    # write_to = write dataframe to supported filetype: csv, json, pkl\n",
    "    \n",
    "    # api call\n",
    "    url = 'https://min-api.cryptocompare.com/data/histoday?fsym={}&tsym={}&limit={}&aggregate={}'\\\n",
    "            .format(symbol.upper(),\n",
    "                    comparison_symbol.upper(),\n",
    "                    rows,\n",
    "                    aggregate)\n",
    "    \n",
    "    if all_data:\n",
    "        url += '&allData=true'\n",
    "    \n",
    "    # store in dataframe\n",
    "    page = requests.get(url)\n",
    "    data = page.json()['Data']\n",
    "    df = pd.DataFrame(data)\n",
    "    df['date'] = [dt.datetime.fromtimestamp(d) for d in df.time]\n",
    "    df['date'] = df.date.dt.date\n",
    "    df['date'] = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "    df.drop('time', axis=1, inplace=True)\n",
    "    df = df[['date', 'open', 'high', 'low', 'close', 'volumefrom', 'volumeto']]\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # write to file option\n",
    "    return_df=True\n",
    "    if write_to is not False:\n",
    "        return_df=False\n",
    "        date = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "        filename = symbol.lower() + '_data_' + date + '.{}'.format(write_to)\n",
    "        write_df(df, write_to, filename)\n",
    "        \n",
    "    if return_df is True:\n",
    "        return df\n",
    "    \n",
    "def fred_data(series_id,\n",
    "              api_key,\n",
    "              file_type='json',\n",
    "              frequency='d',\n",
    "              raw=False,\n",
    "              write_to=False):\n",
    "    \n",
    "    # series_id = string, symbol from fred\n",
    "    # filetype options: xml, json, txt, xls\n",
    "    # frequency options: d = daily, w = weekly, bw = biweekly\n",
    "    # m = monthly, q = quarterly, sa = semiannual, a = annual\n",
    "    # raw=True will skip the preprocessing and return the raw data\n",
    "    # write_to=write dataframe to supported filetype: csv, json, pkl\n",
    "    \n",
    "    url = 'https://api.stlouisfed.org/fred/series/observations?series_id={}&api_key={}&file_type={}&frequency={}'\\\n",
    "                  .format(series_id,\n",
    "                          api_key,\n",
    "                          file_type,\n",
    "                          frequency)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()['observations']\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if raw is False:\n",
    "        df.drop(['realtime_end', 'realtime_start'], axis=1, inplace=True)\n",
    "        df['date'] = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "        df.rename(columns={'value':series_id.lower()}, inplace=True)\n",
    "        \n",
    "    return_df=True\n",
    "    if write_to is not False:\n",
    "        return_df=False\n",
    "        date = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "        filename = series_id.lower() + '_data_' + date + '.{}'.format(write_to)\n",
    "        write_df(df, write_to, filename)\n",
    "        \n",
    "    if return_df is True:\n",
    "        return df\n",
    "\n",
    "def write_df(df, write_to, filename):\n",
    "    if write_to == 'csv':\n",
    "        df.to_csv('{}'.format(filename), index=False)\n",
    "    elif write_to == 'pkl':\n",
    "        df.to_pickle('{}'.format(filename))\n",
    "    elif write_to == 'json':\n",
    "        df.to_json('{}'.format(filename), orient='split')\n",
    "    else:\n",
    "        print('function does not support writing to {}'.format(write_to))\n",
    "        print('format filetype as shown (string in all lower case)')\n",
    "        print('supported filetypes: csv, json, pkl')\n",
    "        \n",
    "def get_file_contents(filename):\n",
    "    \"\"\" Given a filename,\n",
    "        return the contents of that file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coin data\n",
    "coins = ['ETH', 'BTC', 'XRP', 'EOS', 'LTC', 'XLM', 'XMR']\n",
    "\n",
    "# create empty df for each coin\n",
    "coins_d = {coin: pd.DataFrame() for coin in coins}\n",
    "\n",
    "# populate coin dfs with respective coin data\n",
    "for coin, df in coins_d.items():\n",
    "    coins_d[coin] = cryptocompare_data(symbol=coin)\n",
    "    \n",
    "# convert keys:values to variables = assignments\n",
    "locals().update(coins_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fred data\n",
    "fred_apikey = get_file_contents('fred_apikey')\n",
    "\n",
    "indices = {\n",
    "    'VIXCLS':'d', # cboe volatility index\n",
    "    'TWEXB':'w', # trade weighted usd index\n",
    "    'EFFR':'d' # effective federal funds rate\n",
    "    }\n",
    "\n",
    "# create empty df for each index\n",
    "indices_d = {index: pd.DataFrame() for index in indices}\n",
    "\n",
    "# populate dfs with index data\n",
    "for key in set(indices.keys()) and set(indices_d.keys()):\n",
    "    indices_d[key] = fred_data(series_id=key,\n",
    "                               api_key=fred_apikey,\n",
    "                               frequency = indices[key])\n",
    "\n",
    "# convert to variables = assignments\n",
    "locals().update(indices_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update currency column with coin names\n",
    "for coin, df in coins_d.items():\n",
    "    cols_orig = list(coins_d[coin].columns)[1:]\n",
    "    cols_new = ['date']\n",
    "    for col in cols_orig:\n",
    "        cols_new.append('{}_{}'.format(coin, col))\n",
    "    coins_d[coin].columns = cols_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#exog_curr = [btc, xrp, eos, ltc, xlm, xmr]\n",
    "\n",
    "#for curr, symbol in zip(exog_curr, coins[1:]):\n",
    "#    curr.drop(['open', 'high', 'low', 'volumefrom', 'volumeto'],\n",
    "#             axis=1,\n",
    "#             inplace=True)\n",
    "#    curr.rename({'close':symbol}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling & Forecasting Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Time Series with Exogenous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modeling & Forecasting with Exogenous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary & Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
